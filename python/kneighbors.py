# -*- coding: utf-8 -*-
"""kNeighbors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8TRaJ5XuKnHP6X1wiCH0rKZwMSJfIm3
"""

import pandas as pd
from sklearn.impute import SimpleImputer
import seaborn as sb

# URL to CSV file.
url = "https://firebasestorage.googleapis.com/v0/b/allowance-b457c.appspot.com/o/stroke.csv?alt=media&token=a8b85b2f-e9da-43a0-8d66-66e980c7300b"
#Make url into dataset.
dataset = pd.read_csv(url)
#Drop irrelevant variables.
dataset.drop(['id','ever_married','Residence_type'], axis=1, inplace=True)
#Find the null values.
dataset.isnull().sum()
#Use the average of the given variable to fill in the null values.
imp = SimpleImputer(strategy='median')
dataset['bmi'] = imp.fit_transform(dataset['bmi'].to_numpy().reshape(-1, 1))
#Turn strings into numbers.
dataset = pd.get_dummies(dataset, columns = ['gender','smoking_status','work_type'])

dataset.head()
#pair plot
#sb.pairplot(dataset, hue="stroke")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Get all but the last column.
# These are the 'attributes' - the values we are using to make predictions on.
X = dataset.iloc[:, :-1].values


# Get the last column.
# These are the 'labels' - what we are trying to predict.
y = dataset.iloc[:, -1].values


# Split the data up into two groups - a 'train' set and a 'test' set.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)


# Scale the data so all values have equal weight.
scaler = StandardScaler()

# Calculate the scale.
scaler.fit(X_train)

# Apply the scale to the train and test data sets.
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn import neighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import GaussianNB


#number of neighbors.
classifier = KNeighborsClassifier(n_neighbors=5)

# Train the model.
classifier.fit(X_train, y_train)


# Make our predictions.
y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred)
cmd = ConfusionMatrixDisplay(cm, display_labels=['Stroke', 'No Stroke'])
cmd.plot()

import numpy as np
import matplotlib.pyplot as plt

error = []

# Calculating error for K values between 1 and 40.
for i in range(1, 40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error.append(np.mean(pred_i != y_test))

#Graph the K values and mean error.
plt.figure(figsize=(14, 8))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)
plt.title('Error Rate by K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')